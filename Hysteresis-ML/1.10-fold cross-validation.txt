import numpy as np
import pandas as pd
from keras.models import Sequential, clone_model
from keras.layers import Dense, LeakyReLU
from keras.initializers import RandomNormal
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split, KFold
from keras.callbacks import EarlyStopping
import os

# ==== Configuration parameters ====
features = [str(i) for i in range(1, 12)]
y_labels = ['y1', 'y2']
data_path = 'data.xlsx'

test_seed = 37
cv_seed = 71

neurons1_range = range(10, 20)
neurons2_range = range(8, 15)
epochs_range = [200, 300, 400]
lr_range = [0.01, 0.02, 0.05]

# ==== Read data ====
data_df = pd.read_excel(
    data_path,
    header=None,
    names=features + y_labels,
    engine='openpyxl'
)
X = data_df[features].values
y = data_df[y_labels].values

# ==== Evaluation function (after inverse scaling) ====
def evaluate_metrics(y_true, y_pred):
    mae, rmse, r2, mape = [], [], [], []
    for i in range(y_true.shape[1]):
        mae.append(mean_absolute_error(y_true[:, i], y_pred[:, i]))
        rmse.append(np.sqrt(np.mean((y_true[:, i] - y_pred[:, i]) ** 2)))
        r2.append(r2_score(y_true[:, i], y_pred[:, i]))
        mape.append(
            np.mean(np.abs((y_true[:, i] - y_pred[:, i]) /
                           (y_true[:, i] + 1e-8))) * 100
        )
    return mae, rmse, r2, mape

# ==== Build model (linear output) ====
def build_model(neurons1, neurons2, lr):
    model = Sequential([
        Dense(neurons1, input_dim=11, kernel_initializer=RandomNormal()),
        LeakyReLU(alpha=0.01),

        Dense(neurons2, kernel_initializer=RandomNormal()),
        LeakyReLU(alpha=0.01),

        Dense(2)
    ])
    model.compile(
        optimizer=Adam(learning_rate=lr),
        loss='huber_loss'
    )
    return model

# ==== Dataset split ====
X_train_full, X_test, y_train_full, y_test = train_test_split(
    X, y, test_size=0.2, random_state=test_seed
)

# ==== X normalization ====
scaler_X = MinMaxScaler()
X_train_scaled = scaler_X.fit_transform(X_train_full)
X_test_scaled = scaler_X.transform(X_test)

# ==== y normalization (fit on training set only) ====
scaler_y = MinMaxScaler()
y_train_scaled = scaler_y.fit_transform(y_train_full)

# ==== Grid search + 10-fold cross-validation ====
best_r2_avg = -np.inf
best_params = None

for neurons1 in neurons1_range:
    for neurons2 in neurons2_range:
        for lr in lr_range:
            for epochs in epochs_range:

                print(f"\nğŸ” Parameter combination: L1={neurons1}, L2={neurons2}, lr={lr}, epochs={epochs}")

                kf = KFold(n_splits=10, shuffle=True, random_state=cv_seed)
                r2_sum_fold = 0

                for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_scaled), 1):
                    X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]
                    y_tr, y_val = y_train_scaled[train_idx], y_train_scaled[val_idx]

                    model = build_model(neurons1, neurons2, lr)
                    early_stop = EarlyStopping(
                        monitor='val_loss',
                        patience=50,
                        restore_best_weights=True
                    )

                    model.fit(
                        X_tr, y_tr,
                        validation_data=(X_val, y_val),
                        epochs=epochs,
                        batch_size=32,
                        callbacks=[early_stop],
                        verbose=0
                    )

                    # ==== Training set (inverse scaling) ====
                    y_tr_pred = scaler_y.inverse_transform(model.predict(X_tr, verbose=0))
                    y_tr_true = scaler_y.inverse_transform(y_tr)
                    tr_mae, tr_rmse, tr_r2, tr_mape = evaluate_metrics(y_tr_true, y_tr_pred)

                    # ==== Validation set (inverse scaling) ====
                    y_val_pred = scaler_y.inverse_transform(model.predict(X_val, verbose=0))
                    y_val_true = scaler_y.inverse_transform(y_val)
                    val_mae, val_rmse, val_r2, val_mape = evaluate_metrics(y_val_true, y_val_pred)

                    print(f"\n===== Fold {fold} =====")
                    for i, label in enumerate(y_labels):
                        print(
                            f"{label} | "
                            f"Train -> RÂ²={tr_r2[i]:.4f}, MAE={tr_mae[i]:.4f}, RMSE={tr_rmse[i]:.4f}, MAPE={tr_mape[i]:.2f}% | "
                            f"Val -> RÂ²={val_r2[i]:.4f}, MAE={val_mae[i]:.4f}, RMSE={val_rmse[i]:.4f}, MAPE={val_mape[i]:.2f}%"
                        )

                    r2_sum_fold += np.sum(val_r2)

                r2_avg = r2_sum_fold / kf.n_splits
                print(f"\nğŸ“Š Average validation RÂ² = {r2_avg:.5f}")

                if r2_avg > best_r2_avg:
                    best_r2_avg = r2_avg
                    best_params = {
                        'neurons1': neurons1,
                        'neurons2': neurons2,
                        'lr': lr,
                        'epochs': epochs
                    }

print("\nâœ… Best parameters:", best_params)