import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, LeakyReLU, Lambda
from keras.initializers import RandomNormal
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from keras.callbacks import EarlyStopping
import joblib
import os

# ================== 1. Fixed parameters ==================
SEED = 37
NEURONS1 = 19
NEURONS2 = 12
LEARNING_RATE = 0.02
EPOCHS = 200
BATCH_SIZE = 32

features = ['1','2','3','4','5','6','7','8','9','10','11']
y_labels = ['y1', 'y2']
data_path = 'data.xlsx'

# ================== 2. Read data ==================
data_df = pd.read_excel(
    data_path,
    header=None,
    names=features + y_labels,
    engine='openpyxl'
)

X = data_df[features].values
y = data_df[y_labels].values   # y âˆˆ [0, 3]

# ================== 3. Train/test split ==================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=SEED
)

# ================== 4. Normalize only X ==================
scaler_X = MinMaxScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

# ================== 5. Build neural network model ==================
model = Sequential()

model.add(Dense(NEURONS1, input_dim=11, kernel_initializer=RandomNormal()))
model.add(LeakyReLU(alpha=0.01))

model.add(Dense(NEURONS2, kernel_initializer=RandomNormal()))
model.add(LeakyReLU(alpha=0.01))

# Output layer: Sigmoid + Lambda to constrain output in [0, 3]
model.add(Dense(2, activation='sigmoid'))
model.add(Lambda(lambda x: x * 3))

model.compile(
    optimizer=Adam(learning_rate=LEARNING_RATE),
    loss='huber_loss',
    metrics=['mean_absolute_error']
)

# ================== 6. Model training (with EarlyStopping) ==================
early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

model.fit(
    X_train_scaled,
    y_train,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_data=(X_test_scaled, y_test),
    callbacks=[early_stop],
    verbose=1
)

# ================== 7. Model prediction ==================
y_train_pred = model.predict(X_train_scaled)
y_test_pred = model.predict(X_test_scaled)

# ================== 8. Evaluation function ==================
def evaluate(y_true, y_pred):
    r2 = [r2_score(y_true[:, i], y_pred[:, i]) for i in range(y_true.shape[1])]
    mae = [mean_absolute_error(y_true[:, i], y_pred[:, i]) for i in range(y_true.shape[1])]
    return r2, mae

train_r2, train_mae = evaluate(y_train, y_train_pred)
test_r2, test_mae = evaluate(y_test, y_test_pred)

avg_train_r2 = np.mean(train_r2)
avg_test_r2 = np.mean(test_r2)

# ================== 9. Save model and results ==================
save_dir = 'final_model_with_earlystop'
os.makedirs(save_dir, exist_ok=True)

# Save model and scaler
model.save(os.path.join(save_dir, 'model.h5'))
joblib.dump(scaler_X, os.path.join(save_dir, 'scaler_X.save'))

# Save results table
result_df = pd.DataFrame([{
    'neurons1': NEURONS1,
    'neurons2': NEURONS2,
    'learning_rate': LEARNING_RATE,
    'epochs': EPOCHS,
    'train_r2_output1': train_r2[0],
    'train_r2_output2': train_r2[1],
    'test_r2_output1': test_r2[0],
    'test_r2_output2': test_r2[1],
    'train_mae_output1': train_mae[0],
    'train_mae_output2': train_mae[1],
    'test_mae_output1': test_mae[0],
    'test_mae_output2': test_mae[1],
    'avg_train_r2': avg_train_r2,
    'avg_test_r2': avg_test_r2
}])

result_df.to_excel(os.path.join(save_dir, 'result.xlsx'), index=False)

print("âœ… Fixed-parameter model training completed (with EarlyStopping)")
print(f"ðŸ“Š Train Avg RÂ² = {avg_train_r2:.4f}, Test Avg RÂ² = {avg_test_r2:.4f}")